{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9fb2120-dcf8-498b-80ef-f798ce1a8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import *\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, epochs, patience=5, save_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    Train a model with early stopping and learning rate scheduling.\n",
    "    :param model: PyTorch model to train\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param test_loader: DataLoader for testing data\n",
    "    :param criterion: Loss function\n",
    "    :param optimizer: Optimizer\n",
    "    :param epochs: Total number of epochs to train\n",
    "    :param patience: Number of epochs to wait before early stopping (default=5)\n",
    "    :param save_path: Path to save the best model's weights (default='best_model.pth')\n",
    "    \"\"\"\n",
    "    best_loss = float('inf')  # Initialize the best loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "    best_model = None  # Variable to store the best model's weights\n",
    "    \n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = lr_scheduler.LinearLR(\n",
    "        optimizer, start_factor=1.0, end_factor=0.1, total_iters=epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move inputs and targets to the appropriate device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Adjust input dimensions for the model\n",
    "            if inputs.dim() == 2:\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                # Adjust input dimensions for the model\n",
    "                if inputs.dim() == 2:\n",
    "                    inputs = inputs.unsqueeze(1)\n",
    "                inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.unsqueeze(1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        # Early stopping and saving the best model\n",
    "        if avg_test_loss < best_loss:\n",
    "            best_loss = avg_test_loss\n",
    "            patience_counter = 0\n",
    "            best_model = model.state_dict()\n",
    "            torch.save(best_model, save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "    # Restore the best model\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "def get_predictions(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Generate predictions from the model on the test data.\n",
    "    :param model: PyTorch model for prediction\n",
    "    :param test_loader: DataLoader for test data\n",
    "    :param device: Device (e.g., 'cpu' or 'cuda') to run the model\n",
    "    :return: Tuple of predictions and actual values (both as NumPy arrays)\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move inputs to the appropriate device\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Adjust input dimensions for the model\n",
    "            if inputs.dim() == 2:\n",
    "                inputs = inputs.unsqueeze(1)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model(inputs)\n",
    "            predictions.append(outputs.squeeze(-1).cpu())  # Convert to CPU and remove singleton dimension\n",
    "            actuals.append(targets.cpu())  # Convert actual values to CPU\n",
    "\n",
    "    # Convert lists of tensors to NumPy arrays\n",
    "    predictions = torch.cat(predictions).numpy()\n",
    "    actuals = torch.cat(actuals).numpy()\n",
    "\n",
    "    return predictions, actuals\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters and configuration\n",
    "len_cut = 24                # We use hourly data, and our window has length of 24, that is, 1 day\n",
    "kernel_size = 3             # TCN kernel size\n",
    "dilations = [1, 2, 4]       # TCN dilation\n",
    "num_channels = [8, 16, 32]  # Number of channels on each layer\n",
    "learning_rate = 0.01        \n",
    "epochs = 10\n",
    "batch_size = 32 \n",
    "factor = 0.1                # Hyperparameter alpha, tested with from 0.1 to 0.9\n",
    "K = 2                       # SSA window length, tested with from 2 to 12 (since len_cut=24)\n",
    "\n",
    "# Replace with actual cryptocurrency time series\n",
    "original_data = np.random.randn(10000)  \n",
    "\n",
    "# Create input sequences and labels\n",
    "Input = np.array([original_data[i:i + len_cut + 1] for i in range(len(original_data) - len_cut)])\n",
    "label = original_data[len_cut:]\n",
    "\n",
    "# Normalize the input data\n",
    "M = np.max(Input, axis=1)\n",
    "N = np.min(Input, axis=1)\n",
    "input_data = (Input - N[:, None]) / (M - N)[:, None]\n",
    "\n",
    "# Apply KDE and filter the data\n",
    "f1_results = np.array([\n",
    "    kde_pdf_cdf_function(input_data[i], factor)[0](input_data[i]) \n",
    "    for i in range(input_data.shape[0])\n",
    "])\n",
    "filtered_x = f1_results[:, :-1]\n",
    "filtered_y = f1_results[:, -1]\n",
    "\n",
    "# Apply SSA decomposition\n",
    "data = np.array([SSA_decomposition(filtered_x[i], K) for i in range(filtered_x.shape[0])])\n",
    "labels = filtered_y\n",
    "\n",
    "# Prepare the data loaders\n",
    "train_loader, test_loader = prepare_data(data, labels, train_ratio=0.7, batch_size=batch_size)\n",
    "\n",
    "# Initialize the model and move it to the device\n",
    "model = TCNWithAttention(\n",
    "    input_size=data.shape[1],\n",
    "    num_channels=num_channels,\n",
    "    kernel_size=kernel_size,\n",
    "    dilations=dilations\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Get predictions and actual labels from the test set\n",
    "predictions, actuals = get_predictions(model, test_loader, device)\n",
    "\n",
    "# Calculate the total number of labels in the test set\n",
    "total_label_length = get_total_label_length(test_loader)\n",
    "\n",
    "# Generate real predictions\n",
    "pred = pred_generation(original_data, predictions, factor, total_label_length).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f21faa-6513-4807-8f3e-cf75bacb7aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
